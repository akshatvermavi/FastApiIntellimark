{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bd7d33c",
   "metadata": {},
   "source": [
    "# Log and Outputs Dir Config "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0dc399",
   "metadata": {},
   "source": [
    "#### DONT CHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb01fdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from forecasting_pipeline import forecast_pipeline_debug, add_hist_range\n",
    "\n",
    "class SuppressFilter(logging.Filter):\n",
    "    def __init__(self, name=''):\n",
    "        super().__init__(name)\n",
    "\n",
    "    def filter(self, record):\n",
    "        return not record.name.startswith(self.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fa2f99",
   "metadata": {},
   "source": [
    "## Set the current Notebook Path and Experiment Number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a216d96",
   "metadata": {},
   "source": [
    "#### CHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3950cfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\aksha\\FastAPI\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- CONFIGURATION (Stays in the notebook) ---\n",
    "notebook_path ='.' # <- Outputs would be saved here #c2avi\n",
    "os.chdir(notebook_path)\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "EXPERIMENT_NUMBER = \"Template\" # <- Change this for each experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1650ba1",
   "metadata": {},
   "source": [
    "#### DONT CHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07820cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-13 17:04:24 - INFO - Logging configured. Log file: .\\exp_Template_pipeline.log\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Define the log file name and path ---\n",
    "log_filename = f'exp_{EXPERIMENT_NUMBER}_pipeline.log'\n",
    "log_file_path = os.path.join(notebook_path, log_filename)\n",
    "\n",
    "# --- Step 3: Configure the logger ---\n",
    "logger = logging.getLogger()\n",
    "if logger.hasHandlers():\n",
    "    logger.handlers.clear()\n",
    "\n",
    "logger.setLevel(logging.INFO)\n",
    "log_file = open(log_file_path, 'w', buffering=1)\n",
    "\n",
    "# Create file and console handlers\n",
    "file_handler = logging.StreamHandler(log_file)\n",
    "file_handler.setLevel(logging.INFO)\n",
    "console_handler = logging.StreamHandler(sys.stdout)\n",
    "console_handler.setLevel(logging.INFO)\n",
    "\n",
    "# Create and apply the formatter\n",
    "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "file_handler.setFormatter(formatter)\n",
    "console_handler.setFormatter(formatter)\n",
    "\n",
    "# Step 2: Create an instance of our filter for 'cmdstanpy'\n",
    "suppress_cmdstanpy_filter = SuppressFilter('cmdstanpy')\n",
    "suppress_prophet_filter = SuppressFilter('prophet') \n",
    "# Step 3: Add the filter to the handlers\n",
    "# This will stop 'cmdstanpy' logs from reaching both the file and the console output\n",
    "file_handler.addFilter(suppress_cmdstanpy_filter)\n",
    "console_handler.addFilter(suppress_cmdstanpy_filter)\n",
    "file_handler.addFilter(suppress_prophet_filter) #<-- ADD THIS LINE\n",
    "console_handler.addFilter(suppress_prophet_filter) #<-- ADD THIS LINE\n",
    "\n",
    "\n",
    "# Add the handlers to the logger\n",
    "logger.addHandler(file_handler)\n",
    "logger.addHandler(console_handler)\n",
    "\n",
    "logger.info(f\"Logging configured. Log file: {log_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef97920",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb0cea03",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(\"/Users/aksha/FastAPI/Data/Anonymized_PB-full.csv\") #c1avi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e83733",
   "metadata": {},
   "source": [
    "### Ensure Date column is named 'date'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "488e00f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.rename(columns={\"Date\": \"date\"}, inplace=True) # Rename the data's date column to lowercase 'date'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa16d69f",
   "metadata": {},
   "source": [
    "### Main Config: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a7513a",
   "metadata": {},
   "source": [
    "#### CHANGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c3a68a",
   "metadata": {},
   "source": [
    "#### Key Hierarchy and Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fea1be6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THESE AS PER DATA\n",
    "KEY_COLS = ['Channel','Chain','Depot','SubCat','SKU'] #Optional, can use predefined key col also\n",
    "TARGET_COL = 'UnitsSold'\n",
    "\n",
    "# DONT CHANGE THESE (DATE_COL and key_col)\n",
    "DATE_COL = 'date'\n",
    "key_col = 'key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f48389cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONT CHANGE THESE\n",
    "DATE_COL = 'date'\n",
    "key_col = 'key'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2857c64a",
   "metadata": {},
   "source": [
    "### - Creating key\n",
    "### - Creating Product Mapping\n",
    "### - Cleaning Duplicates\n",
    "### - Adding Historical Range values for each key\n",
    "### - Formatting dtypes for input dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30a6ff1",
   "metadata": {},
   "source": [
    "#### DONT CHANGE (IDEALLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa25ce09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aksha\\AppData\\Local\\Temp\\ipykernel_22336\\3639545728.py:14: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n"
     ]
    }
   ],
   "source": [
    "# Nothing to change here as such\n",
    "joiner = lambda x:'_'.join(map(str,x))\n",
    "df_data[key_col]=df_data[KEY_COLS].apply(joiner,axis=1)\n",
    "df_data['key'] = df_data['key'].astype(str)\n",
    "\n",
    "product_mapping = df_data[['key'] + KEY_COLS].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "df = df_data[[key_col, DATE_COL, TARGET_COL]].copy()\n",
    "\n",
    "df.groupby([key_col, DATE_COL])[TARGET_COL].sum().reset_index()\n",
    "\n",
    "df = add_hist_range(df, key_col=key_col, date_col=DATE_COL)\n",
    "\n",
    "#df[DATE_COL] = pd.to_datetime(df[DATE_COL]) #avi3\n",
    "df[DATE_COL] = pd.to_datetime(df[DATE_COL], format='%Y-%m-%d')  \n",
    "df[key_col] = df[key_col].astype(str)\n",
    "df[TARGET_COL] = pd.to_numeric(df[TARGET_COL], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78734446",
   "metadata": {},
   "source": [
    "### Define key-wise Seasonality, each key must have either a 'Y' or 'N' value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad17416",
   "metadata": {},
   "source": [
    "#### CHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc79013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign random 'Y' or 'N' to each unique key  \n",
    "np.random.seed(42)  # For reproducibility\n",
    "key_to_seasonal = {k: np.random.choice(['Y', 'N']) for k in df['key'].unique()}\n",
    "# Map to the dataframe\n",
    "df['seasonal'] = df['key'].map(key_to_seasonal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6239145e",
   "metadata": {},
   "source": [
    "#### DONT CHANGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e86d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Params for Forecast Function, DONT CHANGE THESE\n",
    "parameters = {\n",
    "    \"date_col\": DATE_COL,\n",
    "    \"target_col\": TARGET_COL,\n",
    "    \"key_col\": \"key\",\n",
    "    \"seasonal_col\": \"seasonal\",\n",
    "    \"hist_range_col\": \"hist_range\",\n",
    "    # \"output_dir\": notebook_path\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c9a9b2",
   "metadata": {},
   "source": [
    "# Pipeline Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5e05f3",
   "metadata": {},
   "source": [
    "### CHANGE THE CUTOFFS HERE AND FORECAST HORIZON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29c1d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# logging.getLogger('prophet').setLevel(logging.WARNING)\n",
    "# logging.getLogger('cmdstanpy').setLevel(logging.WARNING)\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "\n",
    "# df_test = df[df['hist_range'] == '<6'].copy().reset_index(drop=True)\n",
    "df_test = df[df['key'].isin(df['key'].sample(2))].copy().reset_index(drop=True)\n",
    "\n",
    "\n",
    "results, detailed_results, feature_importance_df = forecast_pipeline_debug(df = df_test, \n",
    "                                parameters = parameters, \n",
    "                                validation_cutoff=\"2024-05-01\", #Train Uptill (Included)\n",
    "                                test_cutoff=\"2024-08-01\", # Validation Cut-off (Included)\n",
    "                                forecast_cutoff=\"2024-11-01\", # Test Cut-off (Included)\n",
    "                                forecasting_horizon=3 # Number of Months to forecast after forecast_cutoff\n",
    "                )\n",
    "\n",
    "\n",
    "# 2. Save the results from the notebook\n",
    "logger.info(\"Pipeline finished. Saving output files...\")\n",
    "results.to_csv(os.path.join(notebook_path, f\"{EXPERIMENT_NUMBER}_pipeline_output.csv\"), index=False)\n",
    "detailed_results.to_csv(os.path.join(notebook_path, f\"{EXPERIMENT_NUMBER}_detailed_pipeline_output.csv\"), index=False)\n",
    "if not feature_importance_df.empty:\n",
    "    feature_importance_df.to_excel(os.path.join(notebook_path, f\"{EXPERIMENT_NUMBER}_feature_importances.xlsx\"), index=True)\n",
    "\n",
    "logger.info(\"===================================\")\n",
    "logger.info(\">>> FORECASTING PIPELINE COMPLETE <<<\")\n",
    "logger.info(\"===================================\")\n",
    "logging.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b41016d",
   "metadata": {},
   "source": [
    "# Analysis Results as you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acfecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.groupby('date')[['actual_value', \n",
    "                         'fcst_best_raw_model_unadjusted', \n",
    "                         'fcst_best_raw_model_adjusted', \n",
    "                         'fcst_best_adj_model_adjusted'\n",
    "                         ]].sum(min_count=1).plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
